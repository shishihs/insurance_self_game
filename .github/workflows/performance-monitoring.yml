name: Performance Monitoring & Regression Detection

on:
  push:
    branches: [ master, develop ]
  pull_request:
    branches: [ master ]
  schedule:
    # Daily performance baseline check at 3 AM UTC
    - cron: '0 3 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Performance test type'
        required: false
        default: 'full'
        type: choice
        options:
          - full
          - lighthouse
          - load
          - bundle
      baseline_update:
        description: 'Update performance baseline'
        required: false
        default: false
        type: boolean

permissions:
  contents: read
  pages: read
  pull-requests: write

jobs:
  # Bundle size analysis
  bundle-analysis:
    name: Bundle Size Analysis
    runs-on: ubuntu-latest
    outputs:
      current_size: ${{ steps.analysis.outputs.current_size }}
      size_change: ${{ steps.analysis.outputs.size_change }}
    
    steps:
    - name: Checkout current
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: "20"
        
    - name: Setup pnpm
      uses: pnpm/action-setup@v4
      with:
        version: 8
        
    - name: Install dependencies
      run: pnpm install --frozen-lockfile
      
    - name: Build current version
      run: |
        echo "üî® Building current version..."
        pnpm run build
        
    - name: Analyze current bundle
      id: analysis
      run: |
        # Calculate total bundle size
        CURRENT_SIZE=$(du -sb dist | cut -f1)
        echo "current_size=$CURRENT_SIZE" >> $GITHUB_OUTPUT
        
        # Detailed analysis
        echo "## üì¶ Bundle Analysis" >> bundle-report.md
        echo "" >> bundle-report.md
        echo "| File | Size (KB) | Type |" >> bundle-report.md
        echo "|------|-----------|------|" >> bundle-report.md
        
        # Analyze individual files
        find dist -type f -name "*.js" -o -name "*.css" -o -name "*.html" | while read file; do
          size=$(du -k "$file" | cut -f1)
          type=$(echo "$file" | sed 's/.*\.//')
          name=$(basename "$file")
          echo "| $name | $size | $type |" >> bundle-report.md
        done
        
        echo "" >> bundle-report.md
        echo "**Total Bundle Size**: $(echo "scale=1; $CURRENT_SIZE / 1024" | bc) KB" >> bundle-report.md
        
        # Load baseline for comparison
        if [ -f .performance-baseline/bundle-size.txt ]; then
          BASELINE_SIZE=$(cat .performance-baseline/bundle-size.txt)
          SIZE_CHANGE=$(echo "scale=2; ($CURRENT_SIZE - $BASELINE_SIZE) * 100 / $BASELINE_SIZE" | bc)
          echo "size_change=$SIZE_CHANGE" >> $GITHUB_OUTPUT
          
          echo "" >> bundle-report.md
          echo "**Baseline**: $(echo "scale=1; $BASELINE_SIZE / 1024" | bc) KB" >> bundle-report.md
          echo "**Change**: $SIZE_CHANGE%" >> bundle-report.md
          
          # Alert thresholds
          if (( $(echo "$SIZE_CHANGE > 10" | bc -l) )); then
            echo "‚ö†Ô∏è Bundle size increased by $SIZE_CHANGE%" >> bundle-report.md
          elif (( $(echo "$SIZE_CHANGE < -5" | bc -l) )); then
            echo "‚úÖ Bundle size reduced by $(echo "$SIZE_CHANGE * -1" | bc)%" >> bundle-report.md
          fi
        else
          echo "size_change=0" >> $GITHUB_OUTPUT
          echo "" >> bundle-report.md
          echo "**Note**: No baseline found, creating new baseline" >> bundle-report.md
        fi
        
        cat bundle-report.md >> $GITHUB_STEP_SUMMARY
        
    - name: Save current size as baseline
      if: github.ref == 'refs/heads/master' || inputs.baseline_update
      run: |
        mkdir -p .performance-baseline
        echo "${{ steps.analysis.outputs.current_size }}" > .performance-baseline/bundle-size.txt
        
    - name: Upload bundle artifacts
      uses: actions/upload-artifact@v4
      with:
        name: bundle-analysis
        path: |
          dist/
          bundle-report.md
          .performance-baseline/
        retention-days: 30

  # Lighthouse performance audit
  lighthouse-audit:
    name: Lighthouse Performance Audit
    runs-on: ubuntu-latest
    needs: [bundle-analysis]
    outputs:
      performance_score: ${{ steps.lighthouse.outputs.performance_score }}
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: "20"
        
    - name: Setup pnpm
      uses: pnpm/action-setup@v4
      with:
        version: 8
        
    - name: Install dependencies
      run: pnpm install --frozen-lockfile
      
    - name: Build for lighthouse
      run: |
        pnpm run build
        
    - name: Install Lighthouse
      run: |
        npm install -g @lhci/cli@0.12.x lighthouse
        
    - name: Start local server
      run: |
        cd dist
        python3 -m http.server 8080 &
        SERVER_PID=$!
        echo "SERVER_PID=$SERVER_PID" >> $GITHUB_ENV
        sleep 5
        
    - name: Run Lighthouse audit
      id: lighthouse
      run: |
        echo "üîç Running Lighthouse audit..."
        
        # Run Lighthouse
        lighthouse http://localhost:8080 \
          --output json \
          --output html \
          --output-path ./lighthouse-report \
          --chrome-flags="--headless --no-sandbox --disable-dev-shm-usage" \
          --disable-storage-reset
          
        # Extract scores
        PERFORMANCE_SCORE=$(cat lighthouse-report.json | jq '.categories.performance.score * 100')
        ACCESSIBILITY_SCORE=$(cat lighthouse-report.json | jq '.categories.accessibility.score * 100')
        BEST_PRACTICES_SCORE=$(cat lighthouse-report.json | jq '.categories["best-practices"].score * 100')
        SEO_SCORE=$(cat lighthouse-report.json | jq '.categories.seo.score * 100')
        
        echo "performance_score=$PERFORMANCE_SCORE" >> $GITHUB_OUTPUT
        
        # Create performance report
        echo "## üöÄ Lighthouse Performance Report" >> lighthouse-summary.md
        echo "" >> lighthouse-summary.md
        echo "| Category | Score | Status |" >> lighthouse-summary.md
        echo "|----------|-------|--------|" >> lighthouse-summary.md
        echo "| Performance | $PERFORMANCE_SCORE | $([ $(echo "$PERFORMANCE_SCORE >= 90" | bc) -eq 1 ] && echo "‚úÖ" || echo "‚ö†Ô∏è") |" >> lighthouse-summary.md
        echo "| Accessibility | $ACCESSIBILITY_SCORE | $([ $(echo "$ACCESSIBILITY_SCORE >= 90" | bc) -eq 1 ] && echo "‚úÖ" || echo "‚ö†Ô∏è") |" >> lighthouse-summary.md
        echo "| Best Practices | $BEST_PRACTICES_SCORE | $([ $(echo "$BEST_PRACTICES_SCORE >= 90" | bc) -eq 1 ] && echo "‚úÖ" || echo "‚ö†Ô∏è") |" >> lighthouse-summary.md
        echo "| SEO | $SEO_SCORE | $([ $(echo "$SEO_SCORE >= 90" | bc) -eq 1 ] && echo "‚úÖ" || echo "‚ö†Ô∏è") |" >> lighthouse-summary.md
        
        # Extract key metrics
        FCP=$(cat lighthouse-report.json | jq '.audits["first-contentful-paint"].numericValue')
        LCP=$(cat lighthouse-report.json | jq '.audits["largest-contentful-paint"].numericValue')
        TTI=$(cat lighthouse-report.json | jq '.audits["interactive"].numericValue')
        
        echo "" >> lighthouse-summary.md
        echo "### Key Metrics" >> lighthouse-summary.md
        echo "- **First Contentful Paint**: $(echo "scale=1; $FCP / 1000" | bc)s" >> lighthouse-summary.md
        echo "- **Largest Contentful Paint**: $(echo "scale=1; $LCP / 1000" | bc)s" >> lighthouse-summary.md
        echo "- **Time to Interactive**: $(echo "scale=1; $TTI / 1000" | bc)s" >> lighthouse-summary.md
        
        # Performance regression check
        if [ -f .performance-baseline/lighthouse-score.txt ]; then
          BASELINE_SCORE=$(cat .performance-baseline/lighthouse-score.txt)
          SCORE_CHANGE=$(echo "scale=1; $PERFORMANCE_SCORE - $BASELINE_SCORE" | bc)
          
          echo "" >> lighthouse-summary.md
          echo "### Performance Change" >> lighthouse-summary.md
          echo "- **Baseline Score**: $BASELINE_SCORE" >> lighthouse-summary.md
          echo "- **Current Score**: $PERFORMANCE_SCORE" >> lighthouse-summary.md
          echo "- **Change**: $SCORE_CHANGE points" >> lighthouse-summary.md
          
          if (( $(echo "$SCORE_CHANGE < -5" | bc -l) )); then
            echo "" >> lighthouse-summary.md
            echo "‚ö†Ô∏è **Performance regression detected!** Score dropped by $SCORE_CHANGE points." >> lighthouse-summary.md
          fi
        fi
        
        cat lighthouse-summary.md >> $GITHUB_STEP_SUMMARY
        
    - name: Stop server
      if: always()
      run: |
        if [ -n "$SERVER_PID" ]; then
          kill $SERVER_PID || true
        fi
        
    - name: Save performance baseline
      if: github.ref == 'refs/heads/master' || inputs.baseline_update
      run: |
        mkdir -p .performance-baseline
        echo "${{ steps.lighthouse.outputs.performance_score }}" > .performance-baseline/lighthouse-score.txt
        
    - name: Upload lighthouse artifacts
      uses: actions/upload-artifact@v4
      with:
        name: lighthouse-report
        path: |
          lighthouse-report.*
          lighthouse-summary.md
          .performance-baseline/
        retention-days: 30

  # Load testing simulation
  load-testing:
    name: Load Testing Simulation
    runs-on: ubuntu-latest
    if: github.event_name != 'pull_request' || inputs.test_type == 'load'
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: "20"
        
    - name: Setup pnpm
      uses: pnpm/action-setup@v4
      with:
        version: 8
        
    - name: Install dependencies
      run: pnpm install --frozen-lockfile
      
    - name: Build application
      run: pnpm run build
      
    - name: Install testing tools
      run: |
        npm install -g artillery
        
    - name: Create load test config
      run: |
        cat > load-test-config.yml << EOF
        config:
          target: 'http://localhost:8080'
          phases:
            - duration: 60
              arrivalRate: 5
              name: "Warm up"
            - duration: 120
              arrivalRate: 10
              name: "Normal load"
            - duration: 60
              arrivalRate: 20
              name: "Peak load"
        scenarios:
          - name: "Browse game"
            flow:
              - get:
                  url: "/"
              - think: 2
              - get:
                  url: "/health.json"
              - think: 1
        EOF
        
    - name: Start server for load testing
      run: |
        cd dist
        python3 -m http.server 8080 &
        SERVER_PID=$!
        echo "SERVER_PID=$SERVER_PID" >> $GITHUB_ENV
        sleep 5
        
    - name: Run load test
      run: |
        echo "üî• Running load test..."
        artillery run load-test-config.yml --output load-test-results.json
        
        # Generate report
        artillery report load-test-results.json --output load-test-report.html
        
        # Extract key metrics
        echo "## üî• Load Test Results" >> load-test-summary.md
        echo "" >> load-test-summary.md
        
        # Parse results (simplified)
        if [ -f load-test-results.json ]; then
          echo "Load test completed successfully" >> load-test-summary.md
          echo "- Test duration: 4 minutes" >> load-test-summary.md
          echo "- Max concurrent users: 20" >> load-test-summary.md
          echo "- Total requests: ~1200" >> load-test-summary.md
        fi
        
        cat load-test-summary.md >> $GITHUB_STEP_SUMMARY
        
    - name: Stop server
      if: always()
      run: |
        if [ -n "$SERVER_PID" ]; then
          kill $SERVER_PID || true
        fi
        
    - name: Upload load test artifacts
      uses: actions/upload-artifact@v4
      with:
        name: load-test-results
        path: |
          load-test-results.json
          load-test-report.html
          load-test-summary.md
        retention-days: 7

  # Performance regression analysis
  performance-regression:
    name: Performance Regression Analysis
    runs-on: ubuntu-latest
    needs: [bundle-analysis, lighthouse-audit]
    if: always() && github.event_name == 'pull_request'
    
    steps:
    - name: Download baselines
      uses: actions/cache@v4
      with:
        path: .performance-baseline/
        key: performance-baseline-${{ github.base_ref }}
        restore-keys: |
          performance-baseline-master
          performance-baseline-
          
    - name: Regression analysis
      run: |
        echo "üîç Analyzing performance regression..."
        
        BUNDLE_CHANGE="${{ needs.bundle-analysis.outputs.size_change }}"
        PERF_SCORE="${{ needs.lighthouse-audit.outputs.performance_score }}"
        
        echo "## üìä Performance Regression Analysis" >> regression-report.md
        echo "" >> regression-report.md
        
        REGRESSION_DETECTED=false
        
        # Bundle size regression
        if (( $(echo "$BUNDLE_CHANGE > 15" | bc -l) )); then
          echo "‚ùå **Bundle Size Regression**: +$BUNDLE_CHANGE%" >> regression-report.md
          REGRESSION_DETECTED=true
        elif (( $(echo "$BUNDLE_CHANGE > 10" | bc -l) )); then
          echo "‚ö†Ô∏è **Bundle Size Warning**: +$BUNDLE_CHANGE%" >> regression-report.md
        else
          echo "‚úÖ **Bundle Size**: $BUNDLE_CHANGE% (acceptable)" >> regression-report.md
        fi
        
        # Performance score regression
        if [ -f .performance-baseline/lighthouse-score.txt ]; then
          BASELINE_SCORE=$(cat .performance-baseline/lighthouse-score.txt)
          SCORE_CHANGE=$(echo "scale=1; $PERF_SCORE - $BASELINE_SCORE" | bc)
          
          if (( $(echo "$SCORE_CHANGE < -10" | bc -l) )); then
            echo "‚ùå **Performance Score Regression**: $SCORE_CHANGE points" >> regression-report.md
            REGRESSION_DETECTED=true
          elif (( $(echo "$SCORE_CHANGE < -5" | bc -l) )); then
            echo "‚ö†Ô∏è **Performance Score Warning**: $SCORE_CHANGE points" >> regression-report.md
          else
            echo "‚úÖ **Performance Score**: $SCORE_CHANGE points (acceptable)" >> regression-report.md
          fi
        fi
        
        echo "" >> regression-report.md
        if [ "$REGRESSION_DETECTED" = true ]; then
          echo "## üö® Action Required" >> regression-report.md
          echo "" >> regression-report.md
          echo "Performance regression detected. Please:" >> regression-report.md
          echo "1. Review the changes causing performance impact" >> regression-report.md
          echo "2. Consider optimizations or code splitting" >> regression-report.md
          echo "3. Update tests if the regression is acceptable" >> regression-report.md
          echo "" >> regression-report.md
          echo "REGRESSION_DETECTED=true" >> $GITHUB_ENV
        else
          echo "## ‚úÖ Performance Check Passed" >> regression-report.md
          echo "" >> regression-report.md
          echo "No significant performance regression detected." >> regression-report.md
          echo "REGRESSION_DETECTED=false" >> $GITHUB_ENV
        fi
        
        cat regression-report.md >> $GITHUB_STEP_SUMMARY

  # Comment on PR with performance results
  pr-comment:
    name: PR Performance Comment
    runs-on: ubuntu-latest
    needs: [bundle-analysis, lighthouse-audit, performance-regression]
    if: always() && github.event_name == 'pull_request'
    
    steps:
    - name: Comment on PR
      uses: actions/github-script@v7
      with:
        script: |
          const bundleChange = '${{ needs.bundle-analysis.outputs.size_change }}';
          const perfScore = '${{ needs.lighthouse-audit.outputs.performance_score }}';
          const currentSize = '${{ needs.bundle-analysis.outputs.current_size }}';
          
          const comment = `## üìä Performance Impact Report
          
          ### Bundle Analysis
          - **Current Size**: ${Math.round(currentSize / 1024)} KB
          - **Size Change**: ${bundleChange}%
          
          ### Lighthouse Score
          - **Performance Score**: ${perfScore}/100
          
          ### Status
          ${process.env.REGRESSION_DETECTED === 'true' ? '‚ùå Performance regression detected' : '‚úÖ No significant performance impact'}
          
          ---
          <details>
          <summary>üìã Full Performance Report</summary>
          
          For detailed performance metrics, check the workflow artifacts:
          - Bundle analysis report
          - Lighthouse HTML report
          - Performance regression analysis
          
          </details>
          
          *This comment is automatically updated for each commit.*`;
          
          // Find existing comment
          const comments = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
          });
          
          const existingComment = comments.data.find(
            comment => comment.body.includes('## üìä Performance Impact Report')
          );
          
          if (existingComment) {
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: existingComment.id,
              body: comment
            });
          } else {
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: comment
            });
          }