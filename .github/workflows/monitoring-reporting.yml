name: ðŸ“Š Monitoring & Reporting System

on:
  # Continuous monitoring
  schedule:
    - cron: '*/15 * * * *'  # Every 15 minutes
    - cron: '0 */2 * * *'   # Every 2 hours (detailed check)
    - cron: '0 6 * * *'     # Daily at 6 AM UTC (comprehensive report)
  
  # Triggered monitoring
  workflow_run:
    workflows: [
      "ðŸš€ Automated Deployment Pipeline",
      "ðŸ›¡ï¸ Deployment Safety System"
    ]
    types: [completed]
  
  # Manual monitoring
  workflow_dispatch:
    inputs:
      monitoring_type:
        description: 'Type of monitoring to perform'
        required: true
        default: 'comprehensive'
        type: choice
        options:
          - comprehensive
          - performance
          - uptime
          - error-tracking
          - user-analytics
      alert_threshold:
        description: 'Alert threshold level'
        required: false
        default: 'medium'
        type: choice
        options:
          - low
          - medium
          - high
          - critical

permissions:
  contents: write
  pages: read
  actions: read
  issues: write

env:
  MONITORING_URL: "https://shishihs.github.io/insurance_self_game/"
  ALERT_WEBHOOK: ${{ secrets.ALERT_WEBHOOK_URL }}

jobs:
  # Uptime monitoring
  uptime-monitoring:
    name: â±ï¸ Uptime Monitoring
    runs-on: ubuntu-latest
    if: |
      github.event.schedule == '*/15 * * * *' || 
      github.event.inputs.monitoring_type == 'uptime' ||
      github.event.inputs.monitoring_type == 'comprehensive'
    outputs:
      uptime_status: ${{ steps.uptime.outputs.status }}
      response_time: ${{ steps.uptime.outputs.response_time }}
      availability_score: ${{ steps.uptime.outputs.availability_score }}
    steps:
      - name: Uptime check with multiple endpoints
        id: uptime
        timeout-minutes: 5
        run: |
          echo "â±ï¸ Starting comprehensive uptime monitoring..."
          
          BASE_URL="${{ env.MONITORING_URL }}"
          TIMESTAMP=$(date -Iseconds)
          TOTAL_CHECKS=0
          SUCCESSFUL_CHECKS=0
          TOTAL_RESPONSE_TIME=0
          
          # Define endpoints to check
          ENDPOINTS=(
            "$BASE_URL"
            "$BASE_URL/health"
            "$BASE_URL/deploy-info.json"
          )
          
          echo "ðŸ” Checking ${#ENDPOINTS[@]} endpoints..."
          
          # Check each endpoint
          for endpoint in "${ENDPOINTS[@]}"; do
            echo "Checking: $endpoint"
            TOTAL_CHECKS=$((TOTAL_CHECKS + 1))
            
            # Measure response time and check status
            RESPONSE_TIME=$(curl -w "%{time_total}:%{http_code}" -s -o /dev/null "$endpoint" --max-time 10)
            TIME_PART=$(echo "$RESPONSE_TIME" | cut -d: -f1)
            CODE_PART=$(echo "$RESPONSE_TIME" | cut -d: -f2)
            
            if [ "$CODE_PART" = "200" ]; then
              echo "âœ… $endpoint: ${TIME_PART}s (HTTP $CODE_PART)"
              SUCCESSFUL_CHECKS=$((SUCCESSFUL_CHECKS + 1))
              TOTAL_RESPONSE_TIME=$(echo "$TOTAL_RESPONSE_TIME + $TIME_PART" | bc -l)
            else
              echo "âŒ $endpoint: HTTP $CODE_PART"
            fi
          done
          
          # Calculate metrics
          AVAILABILITY_PERCENTAGE=$(echo "scale=2; $SUCCESSFUL_CHECKS * 100 / $TOTAL_CHECKS" | bc -l)
          
          if [ $SUCCESSFUL_CHECKS -gt 0 ]; then
            AVERAGE_RESPONSE_TIME=$(echo "scale=3; $TOTAL_RESPONSE_TIME / $SUCCESSFUL_CHECKS" | bc -l)
          else
            AVERAGE_RESPONSE_TIME=0
          fi
          
          echo "ðŸ“Š Uptime Results:"
          echo "âœ… Successful checks: $SUCCESSFUL_CHECKS/$TOTAL_CHECKS"
          echo "ðŸ“ˆ Availability: $AVAILABILITY_PERCENTAGE%"
          echo "â±ï¸ Average response time: ${AVERAGE_RESPONSE_TIME}s"
          
          # Determine status
          if (( $(echo "$AVAILABILITY_PERCENTAGE >= 95" | bc -l) )); then
            STATUS="excellent"
          elif (( $(echo "$AVAILABILITY_PERCENTAGE >= 90" | bc -l) )); then
            STATUS="good"
          elif (( $(echo "$AVAILABILITY_PERCENTAGE >= 80" | bc -l) )); then
            STATUS="degraded"
          else
            STATUS="critical"
          fi
          
          # Output results
          echo "status=$STATUS" >> $GITHUB_OUTPUT
          echo "response_time=$AVERAGE_RESPONSE_TIME" >> $GITHUB_OUTPUT
          echo "availability_score=$AVAILABILITY_PERCENTAGE" >> $GITHUB_OUTPUT
          
          # Store detailed metrics
          cat > uptime-metrics.json << EOF
          {
            "timestamp": "$TIMESTAMP",
            "total_checks": $TOTAL_CHECKS,
            "successful_checks": $SUCCESSFUL_CHECKS,
            "availability_percentage": $AVAILABILITY_PERCENTAGE,
            "average_response_time": $AVERAGE_RESPONSE_TIME,
            "status": "$STATUS",
            "endpoints_checked": $(printf '%s\n' "${ENDPOINTS[@]}" | jq -R . | jq -s .)
          }
          EOF
          
          echo "ðŸ’¾ Uptime metrics saved"

      - name: Upload uptime metrics
        uses: actions/upload-artifact@v4
        with:
          name: uptime-metrics-${{ github.run_number }}
          path: uptime-metrics.json
          retention-days: 30

  # Performance monitoring
  performance-monitoring:
    name: âš¡ Performance Monitoring
    runs-on: ubuntu-latest
    if: |
      github.event.schedule == '0 */2 * * *' ||
      github.event.inputs.monitoring_type == 'performance' ||
      github.event.inputs.monitoring_type == 'comprehensive'
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install Lighthouse CLI
        run: |
          npm install -g @lhci/cli lighthouse

      - name: Run Lighthouse performance audit
        timeout-minutes: 10
        run: |
          echo "âš¡ Running Lighthouse performance audit..."
          
          BASE_URL="${{ env.MONITORING_URL }}"
          TIMESTAMP=$(date -Iseconds)
          
          # Run Lighthouse
          lighthouse "$BASE_URL" \
            --output json \
            --output-path lighthouse-report.json \
            --chrome-flags="--headless --no-sandbox" \
            --quiet || echo "âš ï¸ Lighthouse completed with warnings"
          
          # Extract key metrics
          if [ -f lighthouse-report.json ]; then
            PERFORMANCE_SCORE=$(cat lighthouse-report.json | jq '.categories.performance.score * 100' | cut -d. -f1)
            ACCESSIBILITY_SCORE=$(cat lighthouse-report.json | jq '.categories.accessibility.score * 100' | cut -d. -f1)
            BEST_PRACTICES_SCORE=$(cat lighthouse-report.json | jq '."categories"."best-practices".score * 100' | cut -d. -f1)
            SEO_SCORE=$(cat lighthouse-report.json | jq '.categories.seo.score * 100' | cut -d. -f1)
            
            # Core Web Vitals
            FCP=$(cat lighthouse-report.json | jq '.audits."first-contentful-paint".numericValue / 1000')
            LCP=$(cat lighthouse-report.json | jq '.audits."largest-contentful-paint".numericValue / 1000')
            CLS=$(cat lighthouse-report.json | jq '.audits."cumulative-layout-shift".numericValue')
            
            echo "ðŸ“Š Performance Scores:"
            echo "âš¡ Performance: $PERFORMANCE_SCORE%"
            echo "â™¿ Accessibility: $ACCESSIBILITY_SCORE%"
            echo "âœ¨ Best Practices: $BEST_PRACTICES_SCORE%"
            echo "ðŸ” SEO: $SEO_SCORE%"
            echo ""
            echo "ðŸŒ Core Web Vitals:"
            echo "ðŸŽ¨ First Contentful Paint: ${FCP}s"
            echo "ðŸ–¼ï¸ Largest Contentful Paint: ${LCP}s" 
            echo "ðŸ“ Cumulative Layout Shift: $CLS"
            
            # Create performance summary
            cat > performance-summary.json << EOF
            {
              "timestamp": "$TIMESTAMP",
              "url": "$BASE_URL",
              "scores": {
                "performance": $PERFORMANCE_SCORE,
                "accessibility": $ACCESSIBILITY_SCORE,
                "best_practices": $BEST_PRACTICES_SCORE,
                "seo": $SEO_SCORE
              },
              "core_web_vitals": {
                "first_contentful_paint": $FCP,
                "largest_contentful_paint": $LCP,
                "cumulative_layout_shift": $CLS
              },
              "commit": "${{ github.sha }}"
            }
            EOF
            
            # Performance status determination
            if [ $PERFORMANCE_SCORE -ge 90 ]; then
              PERF_STATUS="excellent"
            elif [ $PERFORMANCE_SCORE -ge 75 ]; then
              PERF_STATUS="good"
            elif [ $PERFORMANCE_SCORE -ge 50 ]; then
              PERF_STATUS="needs-improvement"
            else
              PERF_STATUS="poor"
            fi
            
            echo "PERFORMANCE_STATUS=$PERF_STATUS" >> $GITHUB_ENV
            echo "PERFORMANCE_SCORE=$PERFORMANCE_SCORE" >> $GITHUB_ENV
          else
            echo "âŒ Lighthouse report not generated"
            echo "PERFORMANCE_STATUS=unknown" >> $GITHUB_ENV
          fi

      - name: Web Vitals measurement
        run: |
          echo "ðŸŒ Measuring additional Web Vitals..."
          
          # Use curl for basic timing measurements
          BASE_URL="${{ env.MONITORING_URL }}"
          
          # DNS resolution time
          DNS_TIME=$(curl -w "%{time_namelookup}" -s -o /dev/null "$BASE_URL" --max-time 10)
          
          # Connection time
          CONNECT_TIME=$(curl -w "%{time_connect}" -s -o /dev/null "$BASE_URL" --max-time 10)
          
          # Time to first byte
          TTFB=$(curl -w "%{time_starttransfer}" -s -o /dev/null "$BASE_URL" --max-time 10)
          
          # Total time
          TOTAL_TIME=$(curl -w "%{time_total}" -s -o /dev/null "$BASE_URL" --max-time 10)
          
          # Download size
          DOWNLOAD_SIZE=$(curl -w "%{size_download}" -s -o /dev/null "$BASE_URL" --max-time 10)
          
          echo "ðŸ” Network Metrics:"
          echo "ðŸŒ DNS Resolution: ${DNS_TIME}s"
          echo "ðŸ”— Connection Time: ${CONNECT_TIME}s"
          echo "âš¡ Time to First Byte: ${TTFB}s"
          echo "â±ï¸ Total Load Time: ${TOTAL_TIME}s"
          echo "ðŸ“¦ Download Size: ${DOWNLOAD_SIZE} bytes"
          
          # Append to performance summary
          cat > network-metrics.json << EOF
          {
            "timestamp": "$(date -Iseconds)",
            "dns_resolution_time": $DNS_TIME,
            "connection_time": $CONNECT_TIME,
            "time_to_first_byte": $TTFB,
            "total_load_time": $TOTAL_TIME,
            "download_size": $DOWNLOAD_SIZE
          }
          EOF

      - name: Upload performance data
        uses: actions/upload-artifact@v4
        with:
          name: performance-monitoring-${{ github.run_number }}
          path: |
            lighthouse-report.json
            performance-summary.json
            network-metrics.json
          retention-days: 30

  # Error tracking and monitoring
  error-monitoring:
    name: ðŸš¨ Error Monitoring
    runs-on: ubuntu-latest
    if: |
      github.event.inputs.monitoring_type == 'error-tracking' ||
      github.event.inputs.monitoring_type == 'comprehensive'
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Install Playwright
        run: |
          npm ci --prefer-offline --no-audit
          npx playwright install chromium --with-deps

      - name: Browser error detection
        timeout-minutes: 10
        run: |
          echo "ðŸš¨ Starting browser error detection..."
          
          BASE_URL="${{ env.MONITORING_URL }}"
          
          # Create error detection script
          cat > error-detection.js << 'EOF'
          const { chromium } = require('playwright');
          
          (async () => {
            const browser = await chromium.launch();
            const page = await browser.newPage();
            
            const errors = [];
            const warnings = [];
            const networkErrors = [];
            
            // Capture console errors
            page.on('console', msg => {
              if (msg.type() === 'error') {
                errors.push({
                  type: 'console_error',
                  message: msg.text(),
                  timestamp: new Date().toISOString()
                });
              } else if (msg.type() === 'warning') {
                warnings.push({
                  type: 'console_warning', 
                  message: msg.text(),
                  timestamp: new Date().toISOString()
                });
              }
            });
            
            // Capture page errors
            page.on('pageerror', error => {
              errors.push({
                type: 'page_error',
                message: error.message,
                stack: error.stack,
                timestamp: new Date().toISOString()
              });
            });
            
            // Capture network failures
            page.on('requestfailed', request => {
              networkErrors.push({
                type: 'network_error',
                url: request.url(),
                failure: request.failure().errorText,
                timestamp: new Date().toISOString()
              });
            });
            
            try {
              console.log('ðŸ” Navigating to:', process.env.BASE_URL);
              await page.goto(process.env.BASE_URL, { waitUntil: 'networkidle' });
              
              // Wait for any delayed errors
              await page.waitForTimeout(5000);
              
              // Try some basic interactions
              await page.click('body');
              await page.waitForTimeout(2000);
              
              // Check for critical elements
              const criticalElements = await page.$$('[data-testid], .game-container, #game, .error');
              console.log(`Found ${criticalElements.length} critical elements`);
              
            } catch (error) {
              errors.push({
                type: 'navigation_error',
                message: error.message,
                timestamp: new Date().toISOString()
              });
            }
            
            await browser.close();
            
            // Output results
            const results = {
              timestamp: new Date().toISOString(),
              url: process.env.BASE_URL,
              errors: errors,
              warnings: warnings,
              network_errors: networkErrors,
              error_count: errors.length,
              warning_count: warnings.length,
              network_error_count: networkErrors.length
            };
            
            console.log('ðŸ“Š Error Detection Results:');
            console.log(`âŒ Errors: ${errors.length}`);
            console.log(`âš ï¸ Warnings: ${warnings.length}`);
            console.log(`ðŸŒ Network Errors: ${networkErrors.length}`);
            
            require('fs').writeFileSync('error-report.json', JSON.stringify(results, null, 2));
            
            if (errors.length > 0) {
              console.log('\nðŸš¨ Detected Errors:');
              errors.forEach((error, index) => {
                console.log(`${index + 1}. [${error.type}] ${error.message}`);
              });
            }
          })();
          EOF
          
          # Run error detection
          BASE_URL="${{ env.MONITORING_URL }}" node error-detection.js

      - name: Analyze error patterns
        run: |
          if [ -f error-report.json ]; then
            echo "ðŸ“Š Analyzing error patterns..."
            
            ERROR_COUNT=$(cat error-report.json | jq '.error_count')
            WARNING_COUNT=$(cat error-report.json | jq '.warning_count')
            NETWORK_ERROR_COUNT=$(cat error-report.json | jq '.network_error_count')
            
            # Determine error severity
            if [ $ERROR_COUNT -eq 0 ]; then
              ERROR_SEVERITY="none"
            elif [ $ERROR_COUNT -le 2 ]; then
              ERROR_SEVERITY="low"
            elif [ $ERROR_COUNT -le 5 ]; then
              ERROR_SEVERITY="medium"
            else
              ERROR_SEVERITY="high"
            fi
            
            echo "ERROR_SEVERITY=$ERROR_SEVERITY" >> $GITHUB_ENV
            echo "ERROR_COUNT=$ERROR_COUNT" >> $GITHUB_ENV
            
            echo "ðŸš¨ Error Analysis:"
            echo "âŒ Total Errors: $ERROR_COUNT (Severity: $ERROR_SEVERITY)"
            echo "âš ï¸ Warnings: $WARNING_COUNT"
            echo "ðŸŒ Network Errors: $NETWORK_ERROR_COUNT"
          else
            echo "âŒ Error report not generated"
            echo "ERROR_SEVERITY=unknown" >> $GITHUB_ENV
          fi

      - name: Upload error monitoring data
        uses: actions/upload-artifact@v4
        with:
          name: error-monitoring-${{ github.run_number }}
          path: error-report.json
          retention-days: 30

  # Comprehensive reporting
  comprehensive-reporting:
    name: ðŸ“‹ Comprehensive Reporting
    runs-on: ubuntu-latest
    needs: [uptime-monitoring, performance-monitoring, error-monitoring]
    if: always()
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download all monitoring data
        uses: actions/download-artifact@v4
        with:
          path: monitoring-data/

      - name: Generate comprehensive report
        run: |
          echo "ðŸ“‹ Generating comprehensive monitoring report..."
          
          TIMESTAMP=$(date -Iseconds)
          
          # Create comprehensive report
          cat > comprehensive-monitoring-report.md << EOF
          # ðŸ“Š Comprehensive Monitoring Report
          
          **Generated:** $TIMESTAMP  
          **Repository:** ${{ github.repository }}  
          **Commit:** ${{ github.sha }}  
          **Site URL:** ${{ env.MONITORING_URL }}
          
          ## ðŸ“ˆ Executive Summary
          
          EOF
          
          # Process uptime data
          if [ -f "monitoring-data/uptime-metrics-*/uptime-metrics.json" ]; then
            echo "Processing uptime data..."
            UPTIME_FILE=$(find monitoring-data -name "uptime-metrics.json" | head -1)
            
            if [ -f "$UPTIME_FILE" ]; then
              AVAILABILITY=$(cat "$UPTIME_FILE" | jq -r '.availability_percentage')
              RESPONSE_TIME=$(cat "$UPTIME_FILE" | jq -r '.average_response_time')
              UPTIME_STATUS=$(cat "$UPTIME_FILE" | jq -r '.status')
              
              cat >> comprehensive-monitoring-report.md << EOF
          ### â±ï¸ Uptime & Availability
          - **Status:** $UPTIME_STATUS
          - **Availability:** $AVAILABILITY%
          - **Average Response Time:** ${RESPONSE_TIME}s
          
          EOF
            fi
          fi
          
          # Process performance data
          if [ -f "monitoring-data/performance-monitoring-*/performance-summary.json" ]; then
            echo "Processing performance data..."
            PERF_FILE=$(find monitoring-data -name "performance-summary.json" | head -1)
            
            if [ -f "$PERF_FILE" ]; then
              PERF_SCORE=$(cat "$PERF_FILE" | jq -r '.scores.performance')
              A11Y_SCORE=$(cat "$PERF_FILE" | jq -r '.scores.accessibility')
              
              cat >> comprehensive-monitoring-report.md << EOF
          ### âš¡ Performance Metrics
          - **Performance Score:** $PERF_SCORE%
          - **Accessibility Score:** $A11Y_SCORE%
          - **Status:** ${{ env.PERFORMANCE_STATUS || 'N/A' }}
          
          EOF
            fi
          fi
          
          # Process error data
          if [ -f "monitoring-data/error-monitoring-*/error-report.json" ]; then
            echo "Processing error data..."
            ERROR_FILE=$(find monitoring-data -name "error-report.json" | head -1)
            
            if [ -f "$ERROR_FILE" ]; then
              ERROR_COUNT=$(cat "$ERROR_FILE" | jq -r '.error_count')
              WARNING_COUNT=$(cat "$ERROR_FILE" | jq -r '.warning_count')
              
              cat >> comprehensive-monitoring-report.md << EOF
          ### ðŸš¨ Error Monitoring
          - **Errors Detected:** $ERROR_COUNT
          - **Warnings:** $WARNING_COUNT
          - **Severity:** ${{ env.ERROR_SEVERITY || 'N/A' }}
          
          EOF
            fi
          fi
          
          # Add recommendations
          cat >> comprehensive-monitoring-report.md << EOF
          ## ðŸ’¡ Recommendations
          
          EOF
          
          # Performance recommendations
          if [ "${{ env.PERFORMANCE_STATUS }}" = "needs-improvement" ] || [ "${{ env.PERFORMANCE_STATUS }}" = "poor" ]; then
            cat >> comprehensive-monitoring-report.md << EOF
          ### âš¡ Performance Improvements
          - Consider optimizing bundle size
          - Implement lazy loading for non-critical resources
          - Review and optimize Core Web Vitals
          
          EOF
          fi
          
          # Error recommendations
          if [ "${{ env.ERROR_SEVERITY }}" = "medium" ] || [ "${{ env.ERROR_SEVERITY }}" = "high" ]; then
            cat >> comprehensive-monitoring-report.md << EOF
          ### ðŸš¨ Error Resolution
          - Review and fix detected console errors
          - Implement better error handling
          - Add error monitoring in production
          
          EOF
          fi
          
          # Add footer
          cat >> comprehensive-monitoring-report.md << EOF
          ## ðŸ”— Resources
          
          - **Live Site:** ${{ env.MONITORING_URL }}
          - **Repository:** https://github.com/${{ github.repository }}
          - **Workflow Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
          
          ---
          *Report generated automatically by GitHub Actions*
          EOF
          
          echo "âœ… Comprehensive report generated"

      - name: Create monitoring dashboard data
        run: |
          echo "ðŸ“Š Creating monitoring dashboard data..."
          
          # Create dashboard JSON data
          cat > monitoring-dashboard.json << EOF
          {
            "last_updated": "$(date -Iseconds)",
            "site_url": "${{ env.MONITORING_URL }}",
            "commit": "${{ github.sha }}",
            "workflow_run": "${{ github.run_id }}",
            "uptime": {
              "status": "${{ needs.uptime-monitoring.outputs.uptime_status || 'unknown' }}",
              "availability": "${{ needs.uptime-monitoring.outputs.availability_score || 'N/A' }}",
              "response_time": "${{ needs.uptime-monitoring.outputs.response_time || 'N/A' }}"
            },
            "performance": {
              "status": "${{ env.PERFORMANCE_STATUS || 'unknown' }}",
              "score": "${{ env.PERFORMANCE_SCORE || 'N/A' }}"
            },
            "errors": {
              "severity": "${{ env.ERROR_SEVERITY || 'unknown' }}",
              "count": "${{ env.ERROR_COUNT || 'N/A' }}"
            }
          }
          EOF

      - name: Upload comprehensive monitoring report
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-monitoring-report-${{ github.run_number }}
          path: |
            comprehensive-monitoring-report.md
            monitoring-dashboard.json
            monitoring-data/
          retention-days: 30

  # Alert system
  alert-system:
    name: ðŸš¨ Alert System
    runs-on: ubuntu-latest
    needs: [uptime-monitoring, performance-monitoring, error-monitoring]
    if: always()
    steps:
      - name: Evaluate alert conditions
        run: |
          echo "ðŸš¨ Evaluating alert conditions..."
          
          # Get monitoring results
          UPTIME_STATUS="${{ needs.uptime-monitoring.outputs.uptime_status }}"
          PERFORMANCE_STATUS="${{ env.PERFORMANCE_STATUS }}"
          ERROR_SEVERITY="${{ env.ERROR_SEVERITY }}"
          ALERT_THRESHOLD="${{ github.event.inputs.alert_threshold || 'medium' }}"
          
          SHOULD_ALERT=false
          ALERT_LEVEL="info"
          ALERT_MESSAGES=()
          
          # Check uptime alerts
          case "$UPTIME_STATUS" in
            "critical")
              SHOULD_ALERT=true
              ALERT_LEVEL="critical"
              ALERT_MESSAGES+=("ðŸ”´ CRITICAL: Site availability is critically low")
              ;;
            "degraded")
              if [ "$ALERT_THRESHOLD" != "critical" ]; then
                SHOULD_ALERT=true
                ALERT_LEVEL="warning"
                ALERT_MESSAGES+=("ðŸŸ¡ WARNING: Site availability is degraded")
              fi
              ;;
          esac
          
          # Check performance alerts
          case "$PERFORMANCE_STATUS" in
            "poor")
              SHOULD_ALERT=true
              [ "$ALERT_LEVEL" != "critical" ] && ALERT_LEVEL="warning"
              ALERT_MESSAGES+=("ðŸŸ¡ WARNING: Performance score is poor")
              ;;
            "needs-improvement")
              if [ "$ALERT_THRESHOLD" = "low" ]; then
                SHOULD_ALERT=true
                ALERT_LEVEL="info"
                ALERT_MESSAGES+=("ðŸ”µ INFO: Performance needs improvement")
              fi
              ;;
          esac
          
          # Check error alerts
          case "$ERROR_SEVERITY" in
            "high")
              SHOULD_ALERT=true
              ALERT_LEVEL="critical"
              ALERT_MESSAGES+=("ðŸ”´ CRITICAL: High number of errors detected")
              ;;
            "medium")
              if [ "$ALERT_THRESHOLD" != "critical" ]; then
                SHOULD_ALERT=true
                [ "$ALERT_LEVEL" != "critical" ] && ALERT_LEVEL="warning"
                ALERT_MESSAGES+=("ðŸŸ¡ WARNING: Multiple errors detected")
              fi
              ;;
          esac
          
          echo "SHOULD_ALERT=$SHOULD_ALERT" >> $GITHUB_ENV
          echo "ALERT_LEVEL=$ALERT_LEVEL" >> $GITHUB_ENV
          
          # Join alert messages
          ALERT_MESSAGE=$(printf '%s\n' "${ALERT_MESSAGES[@]}" | paste -sd '\n' -)
          echo "ALERT_MESSAGE<<EOF" >> $GITHUB_ENV
          echo "$ALERT_MESSAGE" >> $GITHUB_ENV
          echo "EOF" >> $GITHUB_ENV
          
          echo "Alert evaluation complete:"
          echo "Should Alert: $SHOULD_ALERT"
          echo "Alert Level: $ALERT_LEVEL"

      - name: Send alerts
        if: env.SHOULD_ALERT == 'true'
        run: |
          echo "ðŸš¨ Sending alerts..."
          
          # Create GitHub issue for critical alerts
          if [ "${{ env.ALERT_LEVEL }}" = "critical" ]; then
            echo "Creating GitHub issue for critical alert..."
            
            ISSUE_TITLE="ðŸš¨ CRITICAL: Site Monitoring Alert - $(date +%Y-%m-%d)"
            ISSUE_BODY="## ðŸš¨ Critical Monitoring Alert
          
          **Alert Level:** ${{ env.ALERT_LEVEL }}
          **Timestamp:** $(date -Iseconds)
          **Site:** ${{ env.MONITORING_URL }}
          
          ### Issues Detected:
          ${{ env.ALERT_MESSAGE }}
          
          ### Monitoring Details:
          - **Uptime Status:** ${{ needs.uptime-monitoring.outputs.uptime_status }}
          - **Performance Status:** ${{ env.PERFORMANCE_STATUS }}
          - **Error Severity:** ${{ env.ERROR_SEVERITY }}
          
          ### Action Required:
          - [ ] Investigate root cause
          - [ ] Fix identified issues
          - [ ] Verify resolution
          
          **Workflow Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          
            # This would create a GitHub issue (requires appropriate permissions)
            echo "Issue would be created with title: $ISSUE_TITLE"
          fi
          
          # Send Slack notification if webhook is configured
          if [ "${{ env.ALERT_WEBHOOK }}" != "" ]; then
            echo "Sending Slack alert..."
            
            COLOR="good"
            case "${{ env.ALERT_LEVEL }}" in
              "critical") COLOR="danger" ;;
              "warning") COLOR="warning" ;;
              "info") COLOR="good" ;;
            esac
            
            curl -X POST "${{ env.ALERT_WEBHOOK }}" \
              -H 'Content-Type: application/json' \
              -d '{
                "text": "ðŸš¨ Monitoring Alert: ${{ env.ALERT_LEVEL }}",
                "color": "'$COLOR'",
                "blocks": [
                  {
                    "type": "section",
                    "text": {
                      "type": "mrkdwn",
                      "text": "*Monitoring Alert*\n${{ env.ALERT_MESSAGE }}\n\n*Site:* ${{ env.MONITORING_URL }}\n*Time:* $(date -Iseconds)"
                    }
                  }
                ]
              }' || echo "Failed to send Slack alert"
          fi

      - name: Alert summary
        run: |
          echo "=================================================================="
          echo "ðŸš¨ ALERT SYSTEM SUMMARY"
          echo "=================================================================="
          echo "Should Alert: ${{ env.SHOULD_ALERT }}"
          echo "Alert Level: ${{ env.ALERT_LEVEL }}"
          echo "Alert Threshold: ${{ github.event.inputs.alert_threshold || 'medium' }}"
          echo ""
          echo "Monitoring Status:"
          echo "- Uptime: ${{ needs.uptime-monitoring.outputs.uptime_status }}"
          echo "- Performance: ${{ env.PERFORMANCE_STATUS }}"
          echo "- Errors: ${{ env.ERROR_SEVERITY }}"
          echo "=================================================================="